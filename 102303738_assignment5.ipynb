{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu45eHl/Sdr3L+a08t1dWP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDgRkpQRCtcU",
        "outputId": "ecc792c0-bfe8-41a4-9ff2-b6d928bc7b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best learning rate: 0.1\n",
            "Best lambda: 1e-15\n",
            "Best cost: 0.020455433522743573\n",
            "Best R2 score: 0.9590891329545128\n"
          ]
        }
      ],
      "source": [
        "# Q1\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "n_samples = 300\n",
        "base = np.random.rand(n_samples, 1)\n",
        "\n",
        "X = np.hstack([\n",
        "    base,\n",
        "    base + 0.01 * np.random.randn(n_samples, 1),\n",
        "    base + 0.02 * np.random.randn(n_samples, 1),\n",
        "    base + 0.03 * np.random.randn(n_samples, 1),\n",
        "    base + 0.04 * np.random.randn(n_samples, 1),\n",
        "    base + 0.05 * np.random.randn(n_samples, 1),\n",
        "    base + 0.06 * np.random.randn(n_samples, 1)\n",
        "])\n",
        "\n",
        "true_w = np.array([3, -2, 1.5, 0.5, 4, -1, 2])\n",
        "y = X.dot(true_w) + 0.5 * np.random.randn(n_samples)\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "\n",
        "y = y.reshape(-1, 1)\n",
        "scaler_y = StandardScaler()\n",
        "y_scaled = scaler_y.fit_transform(y).flatten()\n",
        "\n",
        "def ridge_gradient_descent(X, y, lr, lam, n_iters=1000):\n",
        "    m, n = X.shape\n",
        "    w = np.zeros(n)\n",
        "    b = 0.0\n",
        "    for _ in range(n_iters):\n",
        "        y_pred = X.dot(w) + b\n",
        "        error = y_pred - y\n",
        "        dw = (1/m) * (X.T.dot(error)) + (lam/m) * w\n",
        "        db = (1/m) * np.sum(error)\n",
        "        w -= lr * dw\n",
        "        b -= lr * db\n",
        "        # Check for NaN values and break if found\n",
        "        if np.isnan(w).any() or np.isnan(b) or np.isinf(w).any() or np.isinf(b):\n",
        "            return np.full(n, np.nan), np.nan, np.inf, -np.inf # Return NaN for weights/bias, inf for cost, -inf for R2\n",
        "\n",
        "    y_pred_final = X.dot(w) + b\n",
        "    cost = (1/(2*m)) * np.sum((y_pred_final - y) ** 2) + (lam/(2*m)) * np.sum(w ** 2)\n",
        "    r2 = r2_score(y, y_pred_final)\n",
        "    return w, b, cost, r2\n",
        "\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "lambdas = [1e-15, 1e-10, 1e-5, 1e-3, 0, 1, 10, 20]\n",
        "\n",
        "best_config = None\n",
        "best_r2 = -np.inf\n",
        "best_cost = np.inf\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for lam in lambdas:\n",
        "        w, b, cost, r2 = ridge_gradient_descent(X_scaled, y_scaled, lr, lam, n_iters=2000)\n",
        "        # Only consider valid results for comparison\n",
        "        if not np.isnan(r2) and ((r2 > best_r2) or (r2 == best_r2 and cost < best_cost)):\n",
        "            best_r2 = r2\n",
        "            best_cost = cost\n",
        "            best_config = (lr, lam, w, b)\n",
        "\n",
        "print(\"Best learning rate:\", best_config[0])\n",
        "print(\"Best lambda:\", best_config[1])\n",
        "print(\"Best cost:\", best_cost)\n",
        "print(\"Best R2 score:\", best_r2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# (a) Load + preprocess\n",
        "url = \"https://gist.githubusercontent.com/keeganhines/59974f1ebef97bbaa44fb19143f90bad/raw/Hitters.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "data = data.dropna()\n",
        "X = data.drop(columns=[\"Salary\"])\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "y = data[\"Salary\"]\n",
        "\n",
        "# (b) Split + scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "# (c) Train & evaluate\n",
        "lin = LinearRegression()\n",
        "lin.fit(X_train_s, y_train)\n",
        "y_pred_lin = lin.predict(X_test_s)\n",
        "\n",
        "ridge = Ridge(alpha=1)\n",
        "ridge.fit(X_train_s, y_train)\n",
        "y_pred_ridge = ridge.predict(X_test_s)\n",
        "\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_train_s, y_train)\n",
        "y_pred_lasso = lasso.predict(X_test_s)\n",
        "\n",
        "print(\"Linear R2:\", r2_score(y_test, y_pred_lin))\n",
        "print(\"Ridge R2:\", r2_score(y_test, y_pred_ridge))\n",
        "print(\"Lasso R2:\", r2_score(y_test, y_pred_lasso))\n",
        "\n",
        "print(\"Linear MSE:\", mean_squared_error(y_test, y_pred_lin))\n",
        "print(\"Ridge MSE:\", mean_squared_error(y_test, y_pred_ridge))\n",
        "print(\"Lasso MSE:\", mean_squared_error(y_test, y_pred_lasso))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_fjbhieMiFK",
        "outputId": "c2af3776-e61a-40d7-858c-a038f58e2a49"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear R2: 0.16769360190025295\n",
            "Ridge R2: 0.17104747376757878\n",
            "Lasso R2: 0.11422403850131013\n",
            "Linear MSE: 150540.93304991836\n",
            "Ridge MSE: 149934.3114963777\n",
            "Lasso MSE: 160212.08057711055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.716e+03, tolerance: 4.367e+03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# Load the California housing dataset as an alternative to Boston\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "ridge_alphas = [0.01, 0.1, 1, 10, 100]\n",
        "ridge_cv = RidgeCV(alphas=ridge_alphas, cv=5, scoring=\"r2\")\n",
        "ridge_cv.fit(X_train_scaled, y_train)\n",
        "y_pred_ridge_cv = ridge_cv.predict(X_test_scaled)\n",
        "\n",
        "print(\"RidgeCV best alpha:\", ridge_cv.alpha_)\n",
        "print(\"RidgeCV R2:\", r2_score(y_test, y_pred_ridge_cv))\n",
        "print(\"RidgeCV MSE:\", mean_squared_error(y_test, y_pred_ridge_cv))\n",
        "print(\"-\" * 30)\n",
        "\n",
        "lasso_cv = LassoCV(alphas=None, cv=5, max_iter=10000, random_state=42)\n",
        "lasso_cv.fit(X_train_scaled, y_train)\n",
        "y_pred_lasso_cv = lasso_cv.predict(X_test_scaled)\n",
        "\n",
        "print(\"LassoCV best alpha:\", lasso_cv.alpha_)\n",
        "print(\"LassoCV R2:\", r2_score(y_test, y_pred_lasso_cv))\n",
        "print(\"LassoCV MSE:\", mean_squared_error(y_test, y_pred_lasso_cv))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aKH2sAaMpO9",
        "outputId": "6b0242ee-3423-455e-a7b7-47c89d8fb897"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RidgeCV best alpha: 0.01\n",
            "RidgeCV R2: 0.5757879873121596\n",
            "RidgeCV MSE: 0.5558912301037886\n",
            "------------------------------\n",
            "LassoCV best alpha: 0.000798519564426035\n",
            "LassoCV R2: 0.5766495309609692\n",
            "LassoCV MSE: 0.554762255571242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "classes = np.unique(y_train)\n",
        "m, n = X_train_s.shape\n",
        "\n",
        "W = np.zeros((len(classes), n))\n",
        "b = np.zeros(len(classes))\n",
        "\n",
        "lr = 0.1\n",
        "iters = 2000\n",
        "\n",
        "for i, c in enumerate(classes):\n",
        "    y_c = (y_train == c).astype(int)\n",
        "    w = np.zeros(n)\n",
        "    bi = 0.0\n",
        "    for _ in range(iters):\n",
        "        z = X_train_s @ w + bi\n",
        "        p = sigmoid(z)\n",
        "        error = p - y_c\n",
        "        dw = (1 / m) * (X_train_s.T @ error)\n",
        "        db = (1 / m) * error.sum()\n",
        "        w -= lr * dw\n",
        "        bi -= lr * db\n",
        "    W[i] = w\n",
        "    b[i] = bi\n",
        "\n",
        "z_test = X_test_s @ W.T + b\n",
        "probs = sigmoid(z_test)\n",
        "y_pred = np.argmax(probs, axis=1)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHIn3wjiNRqC",
        "outputId": "cbde8a2d-b197-4988-d4cc-c332acea8ad1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9\n"
          ]
        }
      ]
    }
  ]
}